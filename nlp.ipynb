{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f85a0b53",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "in tokenization we take a paragraph and convert it into tokens.  \n",
    "these tokens can be Sentences,words.\n",
    "\n",
    "##### Terminologies in Tokenization  \n",
    "Corpus =  a paragraph is called corpus.  \n",
    "Documents = Sentences are called Documents.  \n",
    "Vocabulery = unique words present in the corpus.  \n",
    "Words = All the words of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb25adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ritik\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \" Hello , I am RITIK. Today i have started NLP. Lets code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd5111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e6cfdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt',download_dir=r'C:\\nltk_data')\n",
    "nltk.data.path.append(r'C:\\nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35f18d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Ritik.', \"Today you've fixed the error.\", 'Now everything works fine.']\n"
     ]
    }
   ],
   "source": [
    "#paragraph-->sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "corpus = \"Hello Ritik. Today you've fixed the error. Now everything works fine.\"\n",
    "sentences = sent_tokenize(corpus)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0dbfc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paragraph---> words\n",
    "# senetenc --> words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0004ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Ritik',\n",
       " '.',\n",
       " 'Today',\n",
       " 'you',\n",
       " \"'ve\",\n",
       " 'fixed',\n",
       " 'the',\n",
       " 'error',\n",
       " '.',\n",
       " 'Now',\n",
       " 'everything',\n",
       " 'works',\n",
       " 'fine',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in this you've like words are treated as single quantitiy\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0de45121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another method\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b6b443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Ritik',\n",
       " '.',\n",
       " 'Today',\n",
       " 'you',\n",
       " \"'\",\n",
       " 've',\n",
       " 'fixed',\n",
       " 'the',\n",
       " 'error',\n",
       " '.',\n",
       " 'Now',\n",
       " 'everything',\n",
       " 'works',\n",
       " 'fine',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here you've is treated as two different words.\n",
    "# wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb2619a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another method\n",
    "# here full stop is not treated as separate word except the last full stop\n",
    "from nltk.tokenize import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee8e6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordDetokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0a4ffb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"H e l l o   R i t i k .   T o d a y   y o u' v e   f i x e d   t h e   e r r o r .   N o w   e v e r y t h i n g   w o r k s   f i n e.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6efa3",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "it is a process to reduce to its word stem or root word  \n",
    "eg . going,gone,goes --> go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17717309",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"go\", \"goes\", \"going\", \"gone\",\n",
    "    \"run\", \"runs\", \"running\", \"ran\",\n",
    "    \"eat\", \"eats\", \"eating\", \"ate\", \"eaten\",\n",
    "    \"play\", \"plays\", \"playing\", \"played\",\n",
    "    \"make\", \"makes\", \"making\", \"made\",\n",
    "    \"see\", \"sees\", \"seeing\", \"saw\", \"seen\",\n",
    "    \"take\", \"takes\", \"taking\", \"took\", \"taken\",\n",
    "    \"write\", \"writes\", \"writing\", \"wrote\", \"written\",\n",
    "    \"come\", \"comes\", \"coming\", \"came\",\n",
    "    \"read\", \"reads\", \"reading\", \"read\",\n",
    "    \"walk\", \"walks\", \"walking\", \"walked\",\n",
    "    \"buy\", \"buys\", \"buying\", \"bought\",\n",
    "    \"sleep\", \"sleeps\", \"sleeping\", \"slept\",\n",
    "    \"drink\", \"drinks\", \"drinking\", \"drank\", \"drunk\",\n",
    "    \"give\", \"gives\", \"giving\", \"gave\", \"given\",\n",
    "    \"think\", \"thinks\", \"thinking\", \"thought\",\n",
    "    \"begin\", \"begins\", \"beginning\", \"began\", \"begun\",\n",
    "    \"drive\", \"drives\", \"driving\", \"drove\", \"driven\",\n",
    "    \"speak\", \"speaks\", \"speaking\", \"spoke\", \"spoken\",\n",
    "    \"fly\", \"flies\", \"flying\", \"flew\", \"flown\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d1866",
   "metadata": {},
   "source": [
    "# PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95c4e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. porter stemmmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "837ab8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go-->go\n",
      "goes-->goe\n",
      "going-->go\n",
      "gone-->gone\n",
      "run-->run\n",
      "runs-->run\n",
      "running-->run\n",
      "ran-->ran\n",
      "eat-->eat\n",
      "eats-->eat\n",
      "eating-->eat\n",
      "ate-->ate\n",
      "eaten-->eaten\n",
      "play-->play\n",
      "plays-->play\n",
      "playing-->play\n",
      "played-->play\n",
      "make-->make\n",
      "makes-->make\n",
      "making-->make\n",
      "made-->made\n",
      "see-->see\n",
      "sees-->see\n",
      "seeing-->see\n",
      "saw-->saw\n",
      "seen-->seen\n",
      "take-->take\n",
      "takes-->take\n",
      "taking-->take\n",
      "took-->took\n",
      "taken-->taken\n",
      "write-->write\n",
      "writes-->write\n",
      "writing-->write\n",
      "wrote-->wrote\n",
      "written-->written\n",
      "come-->come\n",
      "comes-->come\n",
      "coming-->come\n",
      "came-->came\n",
      "read-->read\n",
      "reads-->read\n",
      "reading-->read\n",
      "read-->read\n",
      "walk-->walk\n",
      "walks-->walk\n",
      "walking-->walk\n",
      "walked-->walk\n",
      "buy-->buy\n",
      "buys-->buy\n",
      "buying-->buy\n",
      "bought-->bought\n",
      "sleep-->sleep\n",
      "sleeps-->sleep\n",
      "sleeping-->sleep\n",
      "slept-->slept\n",
      "drink-->drink\n",
      "drinks-->drink\n",
      "drinking-->drink\n",
      "drank-->drank\n",
      "drunk-->drunk\n",
      "give-->give\n",
      "gives-->give\n",
      "giving-->give\n",
      "gave-->gave\n",
      "given-->given\n",
      "think-->think\n",
      "thinks-->think\n",
      "thinking-->think\n",
      "thought-->thought\n",
      "begin-->begin\n",
      "begins-->begin\n",
      "beginning-->begin\n",
      "began-->began\n",
      "begun-->begun\n",
      "drive-->drive\n",
      "drives-->drive\n",
      "driving-->drive\n",
      "drove-->drove\n",
      "driven-->driven\n",
      "speak-->speak\n",
      "speaks-->speak\n",
      "speaking-->speak\n",
      "spoke-->spoke\n",
      "spoken-->spoken\n",
      "fly-->fli\n",
      "flies-->fli\n",
      "flying-->fli\n",
      "flew-->flew\n",
      "flown-->flown\n"
     ]
    }
   ],
   "source": [
    "# it does not give proper root word for some words\n",
    "for word in words:\n",
    "    print(word + \"-->\" + stemming.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a133250",
   "metadata": {},
   "source": [
    "# RegexpStemmer\n",
    "it uses regular expression to remove prefix or suffix from a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd42dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer = RegexpStemmer('ing$|s$|able$',min =4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "487450f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('eating')\n",
    "reg_stemmer.stem('programmable')\n",
    "reg_stemmer.stem('eats')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5075caf",
   "metadata": {},
   "source": [
    "## Snowball Stemmer\n",
    "Better than Porter stemmer in some ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f8ab131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snow_stem = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e821b28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go-->go\n",
      "goes-->goe\n",
      "going-->go\n",
      "gone-->gone\n",
      "run-->run\n",
      "runs-->run\n",
      "running-->run\n",
      "ran-->ran\n",
      "eat-->eat\n",
      "eats-->eat\n",
      "eating-->eat\n",
      "ate-->ate\n",
      "eaten-->eaten\n",
      "play-->play\n",
      "plays-->play\n",
      "playing-->play\n",
      "played-->play\n",
      "make-->make\n",
      "makes-->make\n",
      "making-->make\n",
      "made-->made\n",
      "see-->see\n",
      "sees-->see\n",
      "seeing-->see\n",
      "saw-->saw\n",
      "seen-->seen\n",
      "take-->take\n",
      "takes-->take\n",
      "taking-->take\n",
      "took-->took\n",
      "taken-->taken\n",
      "write-->write\n",
      "writes-->write\n",
      "writing-->write\n",
      "wrote-->wrote\n",
      "written-->written\n",
      "come-->come\n",
      "comes-->come\n",
      "coming-->come\n",
      "came-->came\n",
      "read-->read\n",
      "reads-->read\n",
      "reading-->read\n",
      "read-->read\n",
      "walk-->walk\n",
      "walks-->walk\n",
      "walking-->walk\n",
      "walked-->walk\n",
      "buy-->buy\n",
      "buys-->buy\n",
      "buying-->buy\n",
      "bought-->bought\n",
      "sleep-->sleep\n",
      "sleeps-->sleep\n",
      "sleeping-->sleep\n",
      "slept-->slept\n",
      "drink-->drink\n",
      "drinks-->drink\n",
      "drinking-->drink\n",
      "drank-->drank\n",
      "drunk-->drunk\n",
      "give-->give\n",
      "gives-->give\n",
      "giving-->give\n",
      "gave-->gave\n",
      "given-->given\n",
      "think-->think\n",
      "thinks-->think\n",
      "thinking-->think\n",
      "thought-->thought\n",
      "begin-->begin\n",
      "begins-->begin\n",
      "beginning-->begin\n",
      "began-->began\n",
      "begun-->begun\n",
      "drive-->drive\n",
      "drives-->drive\n",
      "driving-->drive\n",
      "drove-->drove\n",
      "driven-->driven\n",
      "speak-->speak\n",
      "speaks-->speak\n",
      "speaking-->speak\n",
      "spoke-->spoke\n",
      "spoken-->spoken\n",
      "fly-->fli\n",
      "flies-->fli\n",
      "flying-->fli\n",
      "flew-->flew\n",
      "flown-->flown\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"-->\" + snow_stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd4f3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d687b12d",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "it is like stemming.the output we will get after lemmatization is called 'lemma',which is a root word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca38c5",
   "metadata": {},
   "source": [
    "##### Wordnet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "619a06f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b5a4448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are multiple pos tags\n",
    "# verb -v \n",
    "# Noun - n\n",
    "# adjective - a\n",
    "#adverb - r\n",
    "lemmatizer.lemmatize('eating',pos = 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54855527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go-->go\n",
      "goes-->go\n",
      "going-->go\n",
      "gone-->go\n",
      "run-->run\n",
      "runs-->run\n",
      "running-->run\n",
      "ran-->run\n",
      "eat-->eat\n",
      "eats-->eat\n",
      "eating-->eat\n",
      "ate-->eat\n",
      "eaten-->eat\n",
      "play-->play\n",
      "plays-->play\n",
      "playing-->play\n",
      "played-->play\n",
      "make-->make\n",
      "makes-->make\n",
      "making-->make\n",
      "made-->make\n",
      "see-->see\n",
      "sees-->see\n",
      "seeing-->see\n",
      "saw-->saw\n",
      "seen-->see\n",
      "take-->take\n",
      "takes-->take\n",
      "taking-->take\n",
      "took-->take\n",
      "taken-->take\n",
      "write-->write\n",
      "writes-->write\n",
      "writing-->write\n",
      "wrote-->write\n",
      "written-->write\n",
      "come-->come\n",
      "comes-->come\n",
      "coming-->come\n",
      "came-->come\n",
      "read-->read\n",
      "reads-->read\n",
      "reading-->read\n",
      "read-->read\n",
      "walk-->walk\n",
      "walks-->walk\n",
      "walking-->walk\n",
      "walked-->walk\n",
      "buy-->buy\n",
      "buys-->buy\n",
      "buying-->buy\n",
      "bought-->buy\n",
      "sleep-->sleep\n",
      "sleeps-->sleep\n",
      "sleeping-->sleep\n",
      "slept-->sleep\n",
      "drink-->drink\n",
      "drinks-->drink\n",
      "drinking-->drink\n",
      "drank-->drink\n",
      "drunk-->drink\n",
      "give-->give\n",
      "gives-->give\n",
      "giving-->give\n",
      "gave-->give\n",
      "given-->give\n",
      "think-->think\n",
      "thinks-->think\n",
      "thinking-->think\n",
      "thought-->think\n",
      "begin-->begin\n",
      "begins-->begin\n",
      "beginning-->begin\n",
      "began-->begin\n",
      "begun-->begin\n",
      "drive-->drive\n",
      "drives-->drive\n",
      "driving-->drive\n",
      "drove-->drive\n",
      "driven-->drive\n",
      "speak-->speak\n",
      "speaks-->speak\n",
      "speaking-->speak\n",
      "spoke-->speak\n",
      "spoken-->speak\n",
      "fly-->fly\n",
      "flies-->fly\n",
      "flying-->fly\n",
      "flew-->fly\n",
      "flown-->fly\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"-->\" + lemmatizer.lemmatize(word,pos = 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225b392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d05adb70",
   "metadata": {},
   "source": [
    "## StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42221371",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eadccfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Good morning everyone. Today, I want to talk about the importance of technology in our daily lives. \n",
    "As we move forward into a digital age, we must understand how deeply technology influences everything we do — from communication and education to business and healthcare.\n",
    "It is not just about having the latest gadgets, but about how we use them to make life better.\n",
    "Thank you all for being here and giving me this opportunity to share my thoughts.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02d02676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a65bc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7347aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7189d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a03d56f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea5988ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords -->filter-->stemming\n",
    "for word in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[word])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[word] = ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bbdd809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good morn everyon .',\n",
       " 'today , i want talk import technolog daili live .',\n",
       " 'as move forward digit age , must understand deepli technolog influenc everyth — commun educ busi healthcar .',\n",
       " 'it latest gadget , use make life better .',\n",
       " 'thank give opportun share thought .']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "600a466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snow_stem = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df2b36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords -->filter-->snowball stemming\n",
    "for word in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[word])\n",
    "    words = [snow_stem.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[word] = ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd444225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good morn everyon .',\n",
       " 'today , want talk import technolog daili live .',\n",
       " 'move forward digit age , must understand deepli technolog influenc everyth — commun educ busi healthcar .',\n",
       " 'latest gadget , use make life better .',\n",
       " 'thank give opportun share thought .']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b6d5d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords -->filter-->snowball stemming\n",
    "for word in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[word])\n",
    "    words = [lemmatizer.lemmatize(word,pos = 'v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[word] = ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "506f4152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good morning everyone .',\n",
       " 'Today , I want talk importance technology daily live .',\n",
       " 'As move forward digital age , must understand deeply technology influence everything — communication education business healthcare .',\n",
       " 'It latest gadgets , use make life better .',\n",
       " 'Thank give opportunity share thoughts .']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe7ef19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91214873",
   "metadata": {},
   "source": [
    "### Parts of Speech Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d237ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5eef535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Good', 'JJ'), ('morning', 'NN'), ('everyone', 'NN'), ('.', '.')]\n",
      "[('Today', 'NN'), (',', ','), ('I', 'PRP'), ('want', 'VBP'), ('talk', 'JJ'), ('importance', 'NN'), ('technology', 'NN'), ('daily', 'JJ'), ('lives', 'NNS'), ('.', '.')]\n",
      "[('As', 'IN'), ('move', 'NN'), ('forward', 'RB'), ('digital', 'JJ'), ('age', 'NN'), (',', ','), ('must', 'MD'), ('understand', 'VB'), ('deeply', 'NN'), ('technology', 'NN'), ('influences', 'NNS'), ('everything', 'NN'), ('—', 'JJ'), ('communication', 'NN'), ('education', 'NN'), ('business', 'NN'), ('healthcare', 'NN'), ('.', '.')]\n",
      "[('It', 'PRP'), ('latest', 'JJS'), ('gadgets', 'NNS'), (',', ','), ('use', 'NN'), ('make', 'VBP'), ('life', 'NN'), ('better', 'RBR'), ('.', '.')]\n",
      "[('Thank', 'NNP'), ('giving', 'VBG'), ('opportunity', 'NN'), ('share', 'NN'), ('thoughts', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# finding pos tags for each word\n",
    "for word in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[word])\n",
    "    words = [ word for word in words if word not in set(stopwords.words('english'))]\n",
    "    pos_tag = nltk.pos_tag(words)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93988140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54333b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5dcd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8ef08c3",
   "metadata": {},
   "source": [
    "## Named entity Recognition\n",
    "it extracts entites like place,location,person,data,time,money.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c8b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "paragraph = \" i am Ritik, i am from Rajasthan. currently it is 9 PM \"\n",
    "words = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a91e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_elements= nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'),\n",
       " ('am', 'VBP'),\n",
       " ('Ritik', 'NNP'),\n",
       " (',', ','),\n",
       " ('i', 'NN'),\n",
       " ('am', 'VBP'),\n",
       " ('from', 'IN'),\n",
       " ('Rajasthan', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('currently', 'RB'),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('9', 'CD'),\n",
       " ('PM', 'NN')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a166397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Error downloading 'maxent_ne_chunker_tab' from\n",
      "[nltk_data]     <https://raw.githubusercontent.com/nltk/nltk_data/gh-\n",
      "[nltk_data]     pages/packages/chunkers/maxent_ne_chunker_tab.zip>:\n",
      "[nltk_data]     <urlopen error [Errno 11001] getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0a25fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ne_chunk(tag_elements).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e930dfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79daa1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fee8a559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "395890ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ritik\\AppData\\Roaming\\nltk_data\\chunkers\\maxent_ne_chunker C:\\Users\\ritik\\AppData\\Roaming\\nltk_data\\corpora\\words C:\\Users\\ritik\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt C:\\Users\\ritik\\AppData\\Roaming\\nltk_data\\taggers\\averaged_perceptron_tagger\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.find(\"chunkers/maxent_ne_chunker\"),\n",
    "nltk.data.find(\"corpora/words\"),\n",
    "nltk.data.find(\"tokenizers/punkt\"),\n",
    "nltk.data.find(\"taggers/averaged_perceptron_tagger\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a66be447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ritik\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e0eb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d962e55",
   "metadata": {},
   "source": [
    "## ONE HOT ENCODING\n",
    "in this we convert text to vectors.  \n",
    "first the unique words are extracted from the text and  \n",
    "then the sentence is converted into 1 and 0 form  \n",
    "The food is good  \n",
    "The food is bad  \n",
    "Unique = The food is good bad\n",
    "sentence1 = [[ 1,0,0,0,0]\n",
    "             [ 0,1,0,0,0]\n",
    "             [ 0,0,1,0,0]\n",
    "             [ 0,0,0,1,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c19660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41ba04b5",
   "metadata": {},
   "source": [
    "#### Advantages\n",
    "easy to implement  \n",
    "#### disadvantages\n",
    "1.sparse matrix(matrix which consists alot of 0 and 1) -> which leeds to overfitting  \n",
    "2.it doesnt give fixed size words  \n",
    "3.no semantic meaning is getting captured  \n",
    "4. this will not work when new word comes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417af62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac02ef03",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "first take the unique words  \n",
    "then count the fequency of unique words  \n",
    "eg   \n",
    "boy is good  \n",
    "girl is good  \n",
    "uniques = boy,good,girl  \n",
    "then count good = 2, boy = 1,girl = 1  \n",
    "now check sentences =  \n",
    "s1 = [ 1,1,0] op = 1  \n",
    "s2=  [ 1,0,1] op = 1  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b71cd8",
   "metadata": {},
   "source": [
    "## N grams\n",
    "lets assume two egs:  \n",
    "The food is good  \n",
    "the food is not good  \n",
    "##### after text preprocessing  \n",
    "food not good  \n",
    "[1 0 1]  \n",
    "[1 1 1]  \n",
    "## using Bigram  \n",
    "here we use 2 combinations  \n",
    "for previous eg  \n",
    "food not good foodgood notgood foodnot  \n",
    "[ 1 0 1 1 0 1]  \n",
    "[ 1 1 1 0 1 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7a185d",
   "metadata": {},
   "source": [
    "## Trigram  \n",
    "if we make combination of 3 words than it becomes trigram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65db9b46",
   "metadata": {},
   "source": [
    "## TF - IDF  \n",
    "( term frequency- inverse document frequency)  \n",
    "tf = no of rep. of word in sentence / no. of words in sentences  \n",
    "idf = loge(no of sentences/no of sentences containing the word)  \n",
    "final tf-idf = tf * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bda3a88",
   "metadata": {},
   "source": [
    "# Word Embeddings  \n",
    "it is a representation of word and used for text analysis.  \n",
    "the representation is a real valued vector that encodes the   \n",
    "meaning of the word in such a way that the word that are closer  \n",
    "in vector space are expected to be similiar in meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2027f9",
   "metadata": {},
   "source": [
    "types of WE = 1. count or freq  2. deep learning trained model  \n",
    "freq = OHE ,BOW,TF- IDF  \n",
    "DLTM = WORD2VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d049a24",
   "metadata": {},
   "source": [
    "11. Word2Vec\n",
    "Cosine Similarity, Distance\n",
    "DNN\n",
    "\n",
    "Word2vec is a technique in natural language processing (NLP) for obtaining vector representations of words. These vectors capture information about the meaning of the word based on the surrounding words.\n",
    "The word2vec algorithm estimates these representations by modeling text in a large corpus.\n",
    "Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence.\n",
    "\n",
    "Word2vec was developed by Tomáš Mikolov and colleagues at Google and published in 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bef9b6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

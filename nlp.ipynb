{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f85a0b53",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "in tokenization we take a paragraph and convert it into tokens.  \n",
    "these tokens can be Sentences,words.\n",
    "\n",
    "##### Terminologies in Tokenization  \n",
    "Corpus =  a paragraph is called corpus.  \n",
    "Documents = Sentences are called Documents.  \n",
    "Vocabulery = unique words present in the corpus.  \n",
    "Words = All the words of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb25adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ritik\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \" Hello , I am RITIK. Today i have started NLP. Lets code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd5111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e6cfdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt',download_dir=r'C:\\nltk_data')\n",
    "nltk.data.path.append(r'C:\\nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35f18d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Ritik.', \"Today you've fixed the error.\", 'Now everything works fine.']\n"
     ]
    }
   ],
   "source": [
    "#paragraph-->sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "corpus = \"Hello Ritik. Today you've fixed the error. Now everything works fine.\"\n",
    "sentences = sent_tokenize(corpus)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0dbfc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paragraph---> words\n",
    "# senetenc --> words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0004ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Ritik',\n",
       " '.',\n",
       " 'Today',\n",
       " 'you',\n",
       " \"'ve\",\n",
       " 'fixed',\n",
       " 'the',\n",
       " 'error',\n",
       " '.',\n",
       " 'Now',\n",
       " 'everything',\n",
       " 'works',\n",
       " 'fine',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in this you've like words are treated as single quantitiy\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0de45121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another method\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b6b443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Ritik',\n",
       " '.',\n",
       " 'Today',\n",
       " 'you',\n",
       " \"'\",\n",
       " 've',\n",
       " 'fixed',\n",
       " 'the',\n",
       " 'error',\n",
       " '.',\n",
       " 'Now',\n",
       " 'everything',\n",
       " 'works',\n",
       " 'fine',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here you've is treated as two different words.\n",
    "# wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb2619a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another method\n",
    "# here full stop is not treated as separate word except the last full stop\n",
    "from nltk.tokenize import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee8e6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordDetokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0a4ffb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"H e l l o   R i t i k .   T o d a y   y o u' v e   f i x e d   t h e   e r r o r .   N o w   e v e r y t h i n g   w o r k s   f i n e.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6efa3",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "it is a process to reduce to its word stem or root word  \n",
    "eg . going,gone,goes --> go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17717309",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"go\", \"goes\", \"going\", \"gone\",\n",
    "    \"run\", \"runs\", \"running\", \"ran\",\n",
    "    \"eat\", \"eats\", \"eating\", \"ate\", \"eaten\",\n",
    "    \"play\", \"plays\", \"playing\", \"played\",\n",
    "    \"make\", \"makes\", \"making\", \"made\",\n",
    "    \"see\", \"sees\", \"seeing\", \"saw\", \"seen\",\n",
    "    \"take\", \"takes\", \"taking\", \"took\", \"taken\",\n",
    "    \"write\", \"writes\", \"writing\", \"wrote\", \"written\",\n",
    "    \"come\", \"comes\", \"coming\", \"came\",\n",
    "    \"read\", \"reads\", \"reading\", \"read\",\n",
    "    \"walk\", \"walks\", \"walking\", \"walked\",\n",
    "    \"buy\", \"buys\", \"buying\", \"bought\",\n",
    "    \"sleep\", \"sleeps\", \"sleeping\", \"slept\",\n",
    "    \"drink\", \"drinks\", \"drinking\", \"drank\", \"drunk\",\n",
    "    \"give\", \"gives\", \"giving\", \"gave\", \"given\",\n",
    "    \"think\", \"thinks\", \"thinking\", \"thought\",\n",
    "    \"begin\", \"begins\", \"beginning\", \"began\", \"begun\",\n",
    "    \"drive\", \"drives\", \"driving\", \"drove\", \"driven\",\n",
    "    \"speak\", \"speaks\", \"speaking\", \"spoke\", \"spoken\",\n",
    "    \"fly\", \"flies\", \"flying\", \"flew\", \"flown\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d1866",
   "metadata": {},
   "source": [
    "# PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95c4e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. porter stemmmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "837ab8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go-->go\n",
      "goes-->goe\n",
      "going-->go\n",
      "gone-->gone\n",
      "run-->run\n",
      "runs-->run\n",
      "running-->run\n",
      "ran-->ran\n",
      "eat-->eat\n",
      "eats-->eat\n",
      "eating-->eat\n",
      "ate-->ate\n",
      "eaten-->eaten\n",
      "play-->play\n",
      "plays-->play\n",
      "playing-->play\n",
      "played-->play\n",
      "make-->make\n",
      "makes-->make\n",
      "making-->make\n",
      "made-->made\n",
      "see-->see\n",
      "sees-->see\n",
      "seeing-->see\n",
      "saw-->saw\n",
      "seen-->seen\n",
      "take-->take\n",
      "takes-->take\n",
      "taking-->take\n",
      "took-->took\n",
      "taken-->taken\n",
      "write-->write\n",
      "writes-->write\n",
      "writing-->write\n",
      "wrote-->wrote\n",
      "written-->written\n",
      "come-->come\n",
      "comes-->come\n",
      "coming-->come\n",
      "came-->came\n",
      "read-->read\n",
      "reads-->read\n",
      "reading-->read\n",
      "read-->read\n",
      "walk-->walk\n",
      "walks-->walk\n",
      "walking-->walk\n",
      "walked-->walk\n",
      "buy-->buy\n",
      "buys-->buy\n",
      "buying-->buy\n",
      "bought-->bought\n",
      "sleep-->sleep\n",
      "sleeps-->sleep\n",
      "sleeping-->sleep\n",
      "slept-->slept\n",
      "drink-->drink\n",
      "drinks-->drink\n",
      "drinking-->drink\n",
      "drank-->drank\n",
      "drunk-->drunk\n",
      "give-->give\n",
      "gives-->give\n",
      "giving-->give\n",
      "gave-->gave\n",
      "given-->given\n",
      "think-->think\n",
      "thinks-->think\n",
      "thinking-->think\n",
      "thought-->thought\n",
      "begin-->begin\n",
      "begins-->begin\n",
      "beginning-->begin\n",
      "began-->began\n",
      "begun-->begun\n",
      "drive-->drive\n",
      "drives-->drive\n",
      "driving-->drive\n",
      "drove-->drove\n",
      "driven-->driven\n",
      "speak-->speak\n",
      "speaks-->speak\n",
      "speaking-->speak\n",
      "spoke-->spoke\n",
      "spoken-->spoken\n",
      "fly-->fli\n",
      "flies-->fli\n",
      "flying-->fli\n",
      "flew-->flew\n",
      "flown-->flown\n"
     ]
    }
   ],
   "source": [
    "# it does not give proper root word for some words\n",
    "for word in words:\n",
    "    print(word + \"-->\" + stemming.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a133250",
   "metadata": {},
   "source": [
    "# RegexpStemmer\n",
    "it uses regular expression to remove prefix or suffix from a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd42dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer = RegexpStemmer('ing$|s$|able$',min =4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "487450f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('eating')\n",
    "reg_stemmer.stem('programmable')\n",
    "reg_stemmer.stem('eats')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5075caf",
   "metadata": {},
   "source": [
    "## Snowball Stemmer\n",
    "Better than Porter stemmer in some ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f8ab131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snow_stem = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e821b28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go-->go\n",
      "goes-->goe\n",
      "going-->go\n",
      "gone-->gone\n",
      "run-->run\n",
      "runs-->run\n",
      "running-->run\n",
      "ran-->ran\n",
      "eat-->eat\n",
      "eats-->eat\n",
      "eating-->eat\n",
      "ate-->ate\n",
      "eaten-->eaten\n",
      "play-->play\n",
      "plays-->play\n",
      "playing-->play\n",
      "played-->play\n",
      "make-->make\n",
      "makes-->make\n",
      "making-->make\n",
      "made-->made\n",
      "see-->see\n",
      "sees-->see\n",
      "seeing-->see\n",
      "saw-->saw\n",
      "seen-->seen\n",
      "take-->take\n",
      "takes-->take\n",
      "taking-->take\n",
      "took-->took\n",
      "taken-->taken\n",
      "write-->write\n",
      "writes-->write\n",
      "writing-->write\n",
      "wrote-->wrote\n",
      "written-->written\n",
      "come-->come\n",
      "comes-->come\n",
      "coming-->come\n",
      "came-->came\n",
      "read-->read\n",
      "reads-->read\n",
      "reading-->read\n",
      "read-->read\n",
      "walk-->walk\n",
      "walks-->walk\n",
      "walking-->walk\n",
      "walked-->walk\n",
      "buy-->buy\n",
      "buys-->buy\n",
      "buying-->buy\n",
      "bought-->bought\n",
      "sleep-->sleep\n",
      "sleeps-->sleep\n",
      "sleeping-->sleep\n",
      "slept-->slept\n",
      "drink-->drink\n",
      "drinks-->drink\n",
      "drinking-->drink\n",
      "drank-->drank\n",
      "drunk-->drunk\n",
      "give-->give\n",
      "gives-->give\n",
      "giving-->give\n",
      "gave-->gave\n",
      "given-->given\n",
      "think-->think\n",
      "thinks-->think\n",
      "thinking-->think\n",
      "thought-->thought\n",
      "begin-->begin\n",
      "begins-->begin\n",
      "beginning-->begin\n",
      "began-->began\n",
      "begun-->begun\n",
      "drive-->drive\n",
      "drives-->drive\n",
      "driving-->drive\n",
      "drove-->drove\n",
      "driven-->driven\n",
      "speak-->speak\n",
      "speaks-->speak\n",
      "speaking-->speak\n",
      "spoke-->spoke\n",
      "spoken-->spoken\n",
      "fly-->fli\n",
      "flies-->fli\n",
      "flying-->fli\n",
      "flew-->flew\n",
      "flown-->flown\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"-->\" + snow_stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd4f3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d687b12d",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "it is like stemming.the output we will get after lemmatization is called 'lemma',which is a root word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca38c5",
   "metadata": {},
   "source": [
    "##### Wordnet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "619a06f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b5a4448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are multiple pos tags\n",
    "# verb -v \n",
    "# Noun - n\n",
    "# adjective - a\n",
    "#adverb - r\n",
    "lemmatizer.lemmatize('eating',pos = 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54855527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go-->go\n",
      "goes-->go\n",
      "going-->go\n",
      "gone-->go\n",
      "run-->run\n",
      "runs-->run\n",
      "running-->run\n",
      "ran-->run\n",
      "eat-->eat\n",
      "eats-->eat\n",
      "eating-->eat\n",
      "ate-->eat\n",
      "eaten-->eat\n",
      "play-->play\n",
      "plays-->play\n",
      "playing-->play\n",
      "played-->play\n",
      "make-->make\n",
      "makes-->make\n",
      "making-->make\n",
      "made-->make\n",
      "see-->see\n",
      "sees-->see\n",
      "seeing-->see\n",
      "saw-->saw\n",
      "seen-->see\n",
      "take-->take\n",
      "takes-->take\n",
      "taking-->take\n",
      "took-->take\n",
      "taken-->take\n",
      "write-->write\n",
      "writes-->write\n",
      "writing-->write\n",
      "wrote-->write\n",
      "written-->write\n",
      "come-->come\n",
      "comes-->come\n",
      "coming-->come\n",
      "came-->come\n",
      "read-->read\n",
      "reads-->read\n",
      "reading-->read\n",
      "read-->read\n",
      "walk-->walk\n",
      "walks-->walk\n",
      "walking-->walk\n",
      "walked-->walk\n",
      "buy-->buy\n",
      "buys-->buy\n",
      "buying-->buy\n",
      "bought-->buy\n",
      "sleep-->sleep\n",
      "sleeps-->sleep\n",
      "sleeping-->sleep\n",
      "slept-->sleep\n",
      "drink-->drink\n",
      "drinks-->drink\n",
      "drinking-->drink\n",
      "drank-->drink\n",
      "drunk-->drink\n",
      "give-->give\n",
      "gives-->give\n",
      "giving-->give\n",
      "gave-->give\n",
      "given-->give\n",
      "think-->think\n",
      "thinks-->think\n",
      "thinking-->think\n",
      "thought-->think\n",
      "begin-->begin\n",
      "begins-->begin\n",
      "beginning-->begin\n",
      "began-->begin\n",
      "begun-->begin\n",
      "drive-->drive\n",
      "drives-->drive\n",
      "driving-->drive\n",
      "drove-->drive\n",
      "driven-->drive\n",
      "speak-->speak\n",
      "speaks-->speak\n",
      "speaking-->speak\n",
      "spoke-->speak\n",
      "spoken-->speak\n",
      "fly-->fly\n",
      "flies-->fly\n",
      "flying-->fly\n",
      "flew-->fly\n",
      "flown-->fly\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"-->\" + lemmatizer.lemmatize(word,pos = 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225b392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d05adb70",
   "metadata": {},
   "source": [
    "## StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42221371",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eadccfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Good morning everyone. Today, I want to talk about the importance of technology in our daily lives. \n",
    "As we move forward into a digital age, we must understand how deeply technology influences everything we do — from communication and education to business and healthcare.\n",
    "It is not just about having the latest gadgets, but about how we use them to make life better.\n",
    "Thank you all for being here and giving me this opportunity to share my thoughts.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02d02676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a65bc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7347aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7189d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a03d56f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea5988ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords -->filter-->stemming\n",
    "for word in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[word])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[word] = ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bbdd809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good morn everyon .',\n",
       " 'today , i want talk import technolog daili live .',\n",
       " 'as move forward digit age , must understand deepli technolog influenc everyth — commun educ busi healthcar .',\n",
       " 'it latest gadget , use make life better .',\n",
       " 'thank give opportun share thought .']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "600a466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snow_stem = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df2b36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords -->filter-->snowball stemming\n",
    "for word in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[word])\n",
    "    words = [snow_stem.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[word] = ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd444225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good morn everyon .',\n",
       " 'today , want talk import technolog daili live .',\n",
       " 'move forward digit age , must understand deepli technolog influenc everyth — commun educ busi healthcar .',\n",
       " 'latest gadget , use make life better .',\n",
       " 'thank give opportun share thought .']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b6d5d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords -->filter-->snowball stemming\n",
    "for word in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[word])\n",
    "    words = [lemmatizer.lemmatize(word,pos = 'v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[word] = ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "506f4152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good morning everyone .',\n",
       " 'Today , I want talk importance technology daily live .',\n",
       " 'As move forward digital age , must understand deeply technology influence everything — communication education business healthcare .',\n",
       " 'It latest gadgets , use make life better .',\n",
       " 'Thank give opportunity share thoughts .']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe7ef19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91214873",
   "metadata": {},
   "source": [
    "### Parts of Speech Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d237ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5eef535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Good', 'JJ'), ('morning', 'NN'), ('everyone', 'NN'), ('.', '.')]\n",
      "[('Today', 'NN'), (',', ','), ('I', 'PRP'), ('want', 'VBP'), ('talk', 'JJ'), ('importance', 'NN'), ('technology', 'NN'), ('daily', 'JJ'), ('lives', 'NNS'), ('.', '.')]\n",
      "[('As', 'IN'), ('move', 'NN'), ('forward', 'RB'), ('digital', 'JJ'), ('age', 'NN'), (',', ','), ('must', 'MD'), ('understand', 'VB'), ('deeply', 'NN'), ('technology', 'NN'), ('influences', 'NNS'), ('everything', 'NN'), ('—', 'JJ'), ('communication', 'NN'), ('education', 'NN'), ('business', 'NN'), ('healthcare', 'NN'), ('.', '.')]\n",
      "[('It', 'PRP'), ('latest', 'JJS'), ('gadgets', 'NNS'), (',', ','), ('use', 'NN'), ('make', 'VBP'), ('life', 'NN'), ('better', 'RBR'), ('.', '.')]\n",
      "[('Thank', 'NNP'), ('giving', 'VBG'), ('opportunity', 'NN'), ('share', 'NN'), ('thoughts', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# finding pos tags for each word\n",
    "for word in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[word])\n",
    "    words = [ word for word in words if word not in set(stopwords.words('english'))]\n",
    "    pos_tag = nltk.pos_tag(words)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93988140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54333b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5dcd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8ef08c3",
   "metadata": {},
   "source": [
    "## Named entity Recognition\n",
    "it extracts entites like place,location,person,data,time,money.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27c8b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "paragraph = \" i am Ritik, i am from Rajasthan. currently it is 9 PM \"\n",
    "words = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a91e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_elements= nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64dbb6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'),\n",
       " ('am', 'VBP'),\n",
       " ('Ritik', 'NNP'),\n",
       " (',', ','),\n",
       " ('i', 'NN'),\n",
       " ('am', 'VBP'),\n",
       " ('from', 'IN'),\n",
       " ('Rajasthan', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('currently', 'RB'),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('9', 'CD'),\n",
       " ('PM', 'NN')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12d91efe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mne_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag_elements\u001b[49m\u001b[43m)\u001b[49m.draw()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ritik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\chunk\\__init__.py:192\u001b[39m, in \u001b[36mne_chunk\u001b[39m\u001b[34m(tagged_tokens, binary)\u001b[39m\n\u001b[32m    190\u001b[39m     chunker = ne_chunker(fmt=\u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     chunker = \u001b[43mne_chunker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m chunker.parse(tagged_tokens)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ritik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\chunk\\__init__.py:174\u001b[39m, in \u001b[36mne_chunker\u001b[39m\u001b[34m(fmt)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mne_chunker\u001b[39m(fmt=\u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    171\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[33;03m    Load NLTK's currently recommended named entity chunker.\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMaxent_NE_Chunker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ritik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\chunk\\named_entity.py:330\u001b[39m, in \u001b[36mMaxent_NE_Chunker.__init__\u001b[39m\u001b[34m(self, fmt)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28mself\u001b[39m._fmt = fmt\n\u001b[32m    329\u001b[39m \u001b[38;5;28mself\u001b[39m._tab_dir = find(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchunkers/maxent_ne_chunker_tab/english_ace_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfmt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ritik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\chunk\\named_entity.py:335\u001b[39m, in \u001b[36mMaxent_NE_Chunker.load_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassify\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmaxent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BinaryMaxentFeatureEncoding, load_maxent_params\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     wgt, mpg, lab, aon = \u001b[43mload_maxent_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tab_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     mc = MaxentClassifier(\n\u001b[32m    337\u001b[39m         BinaryMaxentFeatureEncoding(lab, mpg, alwayson_features=aon), wgt\n\u001b[32m    338\u001b[39m     )\n\u001b[32m    339\u001b[39m     \u001b[38;5;28mself\u001b[39m._tagger = NEChunkParserTagger(classifier=mc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ritik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\classify\\maxent.py:1565\u001b[39m, in \u001b[36mload_maxent_params\u001b[39m\u001b[34m(tab_dir)\u001b[39m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_maxent_params\u001b[39m(tab_dir):\n\u001b[32m-> \u001b[39m\u001b[32m1565\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\n\u001b[32m   1567\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtabdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MaxentDecoder\n\u001b[32m   1569\u001b[39m     mdec = MaxentDecoder()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    " \n",
    "nltk.ne_chunk(tag_elements).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a166397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Error downloading 'maxent_ne_chunker_tab' from\n",
      "[nltk_data]     <https://raw.githubusercontent.com/nltk/nltk_data/gh-\n",
      "[nltk_data]     pages/packages/chunkers/maxent_ne_chunker_tab.zip>:\n",
      "[nltk_data]     <urlopen error [Errno 11001] getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e930dfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79daa1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fee8a559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "395890ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ritik\\AppData\\Roaming\\nltk_data\\chunkers\\maxent_ne_chunker C:\\Users\\ritik\\AppData\\Roaming\\nltk_data\\corpora\\words C:\\Users\\ritik\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt C:\\Users\\ritik\\AppData\\Roaming\\nltk_data\\taggers\\averaged_perceptron_tagger\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.find(\"chunkers/maxent_ne_chunker\"),\n",
    "nltk.data.find(\"corpora/words\"),\n",
    "nltk.data.find(\"tokenizers/punkt\"),\n",
    "nltk.data.find(\"taggers/averaged_perceptron_tagger\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66be447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f85a0b53",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "in tokenization we take a paragraph and convert it into tokens.  \n",
    "these tokens can be Sentences,words.\n",
    "\n",
    "##### Terminologies in Tokenization  \n",
    "Corpus =  a paragraph is called corpus.  \n",
    "Documents = Sentences are called Documents.  \n",
    "Vocabulery = unique words present in the corpus.  \n",
    "Words = All the words of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb25adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ritik\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \" Hello , I am RITIK. Today i have started NLP. Lets code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd5111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e6cfdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt',download_dir=r'C:\\nltk_data')\n",
    "nltk.data.path.append(r'C:\\nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35f18d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Ritik.', \"Today you've fixed the error.\", 'Now everything works fine.']\n"
     ]
    }
   ],
   "source": [
    "#paragraph-->sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "corpus = \"Hello Ritik. Today you've fixed the error. Now everything works fine.\"\n",
    "sentences = sent_tokenize(corpus)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0dbfc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paragraph---> words\n",
    "# senetenc --> words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0004ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Ritik',\n",
       " '.',\n",
       " 'Today',\n",
       " 'you',\n",
       " \"'ve\",\n",
       " 'fixed',\n",
       " 'the',\n",
       " 'error',\n",
       " '.',\n",
       " 'Now',\n",
       " 'everything',\n",
       " 'works',\n",
       " 'fine',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in this you've like words are treated as single quantitiy\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0de45121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another method\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b6b443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Ritik',\n",
       " '.',\n",
       " 'Today',\n",
       " 'you',\n",
       " \"'\",\n",
       " 've',\n",
       " 'fixed',\n",
       " 'the',\n",
       " 'error',\n",
       " '.',\n",
       " 'Now',\n",
       " 'everything',\n",
       " 'works',\n",
       " 'fine',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here you've is treated as two different words.\n",
    "# wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb2619a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another method\n",
    "# here full stop is not treated as separate word except the last full stop\n",
    "from nltk.tokenize import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee8e6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordDetokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0a4ffb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"H e l l o   R i t i k .   T o d a y   y o u' v e   f i x e d   t h e   e r r o r .   N o w   e v e r y t h i n g   w o r k s   f i n e.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6efa3",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "it is a process to reduce to its word stem or root word  \n",
    "eg . going,gone,goes --> go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17717309",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"go\", \"goes\", \"going\", \"gone\",\n",
    "    \"run\", \"runs\", \"running\", \"ran\",\n",
    "    \"eat\", \"eats\", \"eating\", \"ate\", \"eaten\",\n",
    "    \"play\", \"plays\", \"playing\", \"played\",\n",
    "    \"make\", \"makes\", \"making\", \"made\",\n",
    "    \"see\", \"sees\", \"seeing\", \"saw\", \"seen\",\n",
    "    \"take\", \"takes\", \"taking\", \"took\", \"taken\",\n",
    "    \"write\", \"writes\", \"writing\", \"wrote\", \"written\",\n",
    "    \"come\", \"comes\", \"coming\", \"came\",\n",
    "    \"read\", \"reads\", \"reading\", \"read\",\n",
    "    \"walk\", \"walks\", \"walking\", \"walked\",\n",
    "    \"buy\", \"buys\", \"buying\", \"bought\",\n",
    "    \"sleep\", \"sleeps\", \"sleeping\", \"slept\",\n",
    "    \"drink\", \"drinks\", \"drinking\", \"drank\", \"drunk\",\n",
    "    \"give\", \"gives\", \"giving\", \"gave\", \"given\",\n",
    "    \"think\", \"thinks\", \"thinking\", \"thought\",\n",
    "    \"begin\", \"begins\", \"beginning\", \"began\", \"begun\",\n",
    "    \"drive\", \"drives\", \"driving\", \"drove\", \"driven\",\n",
    "    \"speak\", \"speaks\", \"speaking\", \"spoke\", \"spoken\",\n",
    "    \"fly\", \"flies\", \"flying\", \"flew\", \"flown\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d1866",
   "metadata": {},
   "source": [
    "# PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95c4e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. porter stemmmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "837ab8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go-->go\n",
      "goes-->goe\n",
      "going-->go\n",
      "gone-->gone\n",
      "run-->run\n",
      "runs-->run\n",
      "running-->run\n",
      "ran-->ran\n",
      "eat-->eat\n",
      "eats-->eat\n",
      "eating-->eat\n",
      "ate-->ate\n",
      "eaten-->eaten\n",
      "play-->play\n",
      "plays-->play\n",
      "playing-->play\n",
      "played-->play\n",
      "make-->make\n",
      "makes-->make\n",
      "making-->make\n",
      "made-->made\n",
      "see-->see\n",
      "sees-->see\n",
      "seeing-->see\n",
      "saw-->saw\n",
      "seen-->seen\n",
      "take-->take\n",
      "takes-->take\n",
      "taking-->take\n",
      "took-->took\n",
      "taken-->taken\n",
      "write-->write\n",
      "writes-->write\n",
      "writing-->write\n",
      "wrote-->wrote\n",
      "written-->written\n",
      "come-->come\n",
      "comes-->come\n",
      "coming-->come\n",
      "came-->came\n",
      "read-->read\n",
      "reads-->read\n",
      "reading-->read\n",
      "read-->read\n",
      "walk-->walk\n",
      "walks-->walk\n",
      "walking-->walk\n",
      "walked-->walk\n",
      "buy-->buy\n",
      "buys-->buy\n",
      "buying-->buy\n",
      "bought-->bought\n",
      "sleep-->sleep\n",
      "sleeps-->sleep\n",
      "sleeping-->sleep\n",
      "slept-->slept\n",
      "drink-->drink\n",
      "drinks-->drink\n",
      "drinking-->drink\n",
      "drank-->drank\n",
      "drunk-->drunk\n",
      "give-->give\n",
      "gives-->give\n",
      "giving-->give\n",
      "gave-->gave\n",
      "given-->given\n",
      "think-->think\n",
      "thinks-->think\n",
      "thinking-->think\n",
      "thought-->thought\n",
      "begin-->begin\n",
      "begins-->begin\n",
      "beginning-->begin\n",
      "began-->began\n",
      "begun-->begun\n",
      "drive-->drive\n",
      "drives-->drive\n",
      "driving-->drive\n",
      "drove-->drove\n",
      "driven-->driven\n",
      "speak-->speak\n",
      "speaks-->speak\n",
      "speaking-->speak\n",
      "spoke-->spoke\n",
      "spoken-->spoken\n",
      "fly-->fli\n",
      "flies-->fli\n",
      "flying-->fli\n",
      "flew-->flew\n",
      "flown-->flown\n"
     ]
    }
   ],
   "source": [
    "# it does not give proper root word for some words\n",
    "for word in words:\n",
    "    print(word + \"-->\" + stemming.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a133250",
   "metadata": {},
   "source": [
    "# RegexpStemmer\n",
    "it uses regular expression to remove prefix or suffix from a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd42dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer = RegexpStemmer('ing$|s$|able$',min =4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "487450f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('eating')\n",
    "reg_stemmer.stem('programmable')\n",
    "reg_stemmer.stem('eats')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5075caf",
   "metadata": {},
   "source": [
    "## Snowball Stemmer\n",
    "Better than Porter stemmer in some ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f8ab131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snow_stem = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e821b28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go-->go\n",
      "goes-->goe\n",
      "going-->go\n",
      "gone-->gone\n",
      "run-->run\n",
      "runs-->run\n",
      "running-->run\n",
      "ran-->ran\n",
      "eat-->eat\n",
      "eats-->eat\n",
      "eating-->eat\n",
      "ate-->ate\n",
      "eaten-->eaten\n",
      "play-->play\n",
      "plays-->play\n",
      "playing-->play\n",
      "played-->play\n",
      "make-->make\n",
      "makes-->make\n",
      "making-->make\n",
      "made-->made\n",
      "see-->see\n",
      "sees-->see\n",
      "seeing-->see\n",
      "saw-->saw\n",
      "seen-->seen\n",
      "take-->take\n",
      "takes-->take\n",
      "taking-->take\n",
      "took-->took\n",
      "taken-->taken\n",
      "write-->write\n",
      "writes-->write\n",
      "writing-->write\n",
      "wrote-->wrote\n",
      "written-->written\n",
      "come-->come\n",
      "comes-->come\n",
      "coming-->come\n",
      "came-->came\n",
      "read-->read\n",
      "reads-->read\n",
      "reading-->read\n",
      "read-->read\n",
      "walk-->walk\n",
      "walks-->walk\n",
      "walking-->walk\n",
      "walked-->walk\n",
      "buy-->buy\n",
      "buys-->buy\n",
      "buying-->buy\n",
      "bought-->bought\n",
      "sleep-->sleep\n",
      "sleeps-->sleep\n",
      "sleeping-->sleep\n",
      "slept-->slept\n",
      "drink-->drink\n",
      "drinks-->drink\n",
      "drinking-->drink\n",
      "drank-->drank\n",
      "drunk-->drunk\n",
      "give-->give\n",
      "gives-->give\n",
      "giving-->give\n",
      "gave-->gave\n",
      "given-->given\n",
      "think-->think\n",
      "thinks-->think\n",
      "thinking-->think\n",
      "thought-->thought\n",
      "begin-->begin\n",
      "begins-->begin\n",
      "beginning-->begin\n",
      "began-->began\n",
      "begun-->begun\n",
      "drive-->drive\n",
      "drives-->drive\n",
      "driving-->drive\n",
      "drove-->drove\n",
      "driven-->driven\n",
      "speak-->speak\n",
      "speaks-->speak\n",
      "speaking-->speak\n",
      "spoke-->spoke\n",
      "spoken-->spoken\n",
      "fly-->fli\n",
      "flies-->fli\n",
      "flying-->fli\n",
      "flew-->flew\n",
      "flown-->flown\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"-->\" + snow_stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd4f3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d687b12d",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "it is like stemming.the output we will get after lemmatization is called 'lemma',which is a root word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca38c5",
   "metadata": {},
   "source": [
    "##### Wordnet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "619a06f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b5a4448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are multiple pos tags\n",
    "# verb -v \n",
    "# Noun - n\n",
    "# adjective - a\n",
    "#adverb - r\n",
    "lemmatizer.lemmatize('eating',pos = 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54855527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go-->go\n",
      "goes-->go\n",
      "going-->go\n",
      "gone-->go\n",
      "run-->run\n",
      "runs-->run\n",
      "running-->run\n",
      "ran-->run\n",
      "eat-->eat\n",
      "eats-->eat\n",
      "eating-->eat\n",
      "ate-->eat\n",
      "eaten-->eat\n",
      "play-->play\n",
      "plays-->play\n",
      "playing-->play\n",
      "played-->play\n",
      "make-->make\n",
      "makes-->make\n",
      "making-->make\n",
      "made-->make\n",
      "see-->see\n",
      "sees-->see\n",
      "seeing-->see\n",
      "saw-->saw\n",
      "seen-->see\n",
      "take-->take\n",
      "takes-->take\n",
      "taking-->take\n",
      "took-->take\n",
      "taken-->take\n",
      "write-->write\n",
      "writes-->write\n",
      "writing-->write\n",
      "wrote-->write\n",
      "written-->write\n",
      "come-->come\n",
      "comes-->come\n",
      "coming-->come\n",
      "came-->come\n",
      "read-->read\n",
      "reads-->read\n",
      "reading-->read\n",
      "read-->read\n",
      "walk-->walk\n",
      "walks-->walk\n",
      "walking-->walk\n",
      "walked-->walk\n",
      "buy-->buy\n",
      "buys-->buy\n",
      "buying-->buy\n",
      "bought-->buy\n",
      "sleep-->sleep\n",
      "sleeps-->sleep\n",
      "sleeping-->sleep\n",
      "slept-->sleep\n",
      "drink-->drink\n",
      "drinks-->drink\n",
      "drinking-->drink\n",
      "drank-->drink\n",
      "drunk-->drink\n",
      "give-->give\n",
      "gives-->give\n",
      "giving-->give\n",
      "gave-->give\n",
      "given-->give\n",
      "think-->think\n",
      "thinks-->think\n",
      "thinking-->think\n",
      "thought-->think\n",
      "begin-->begin\n",
      "begins-->begin\n",
      "beginning-->begin\n",
      "began-->begin\n",
      "begun-->begin\n",
      "drive-->drive\n",
      "drives-->drive\n",
      "driving-->drive\n",
      "drove-->drive\n",
      "driven-->drive\n",
      "speak-->speak\n",
      "speaks-->speak\n",
      "speaking-->speak\n",
      "spoke-->speak\n",
      "spoken-->speak\n",
      "fly-->fly\n",
      "flies-->fly\n",
      "flying-->fly\n",
      "flew-->fly\n",
      "flown-->fly\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"-->\" + lemmatizer.lemmatize(word,pos = 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225b392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d05adb70",
   "metadata": {},
   "source": [
    "## StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42221371",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eadccfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Good morning everyone. Today, I want to talk about the importance of technology in our daily lives. \n",
    "As we move forward into a digital age, we must understand how deeply technology influences everything we do — from communication and education to business and healthcare.\n",
    "It is not just about having the latest gadgets, but about how we use them to make life better.\n",
    "Thank you all for being here and giving me this opportunity to share my thoughts.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02d02676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a65bc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7347aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7189d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a03d56f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea5988ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords -->filter-->stemming\n",
    "for word in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[word])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[word] = ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bbdd809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good morn everyon .',\n",
       " 'today , i want talk import technolog daili live .',\n",
       " 'as move forward digit age , must understand deepli technolog influenc everyth — commun educ busi healthcar .',\n",
       " 'it latest gadget , use make life better .',\n",
       " 'thank give opportun share thought .']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "600a466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snow_stem = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df2b36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords -->filter-->snowball stemming\n",
    "for word in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[word])\n",
    "    words = [snow_stem.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[word] = ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd444225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good morn everyon .',\n",
       " 'today , want talk import technolog daili live .',\n",
       " 'move forward digit age , must understand deepli technolog influenc everyth — commun educ busi healthcar .',\n",
       " 'latest gadget , use make life better .',\n",
       " 'thank give opportun share thought .']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b6d5d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords -->filter-->snowball stemming\n",
    "for word in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[word])\n",
    "    words = [lemmatizer.lemmatize(word,pos = 'v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[word] = ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "506f4152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good morning everyone .',\n",
       " 'Today , I want talk importance technology daily live .',\n",
       " 'As move forward digital age , must understand deeply technology influence everything — communication education business healthcare .',\n",
       " 'It latest gadgets , use make life better .',\n",
       " 'Thank give opportunity share thoughts .']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe7ef19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

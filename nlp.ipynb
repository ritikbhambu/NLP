{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f85a0b53",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "in tokenization we take a paragraph and convert it into tokens.  \n",
    "these tokens can be Sentences,words.\n",
    "\n",
    "##### Terminologies in Tokenization  \n",
    "Corpus =  a paragraph is called corpus.  \n",
    "Documents = Sentences are called Documents.  \n",
    "Vocabulery = unique words present in the corpus.  \n",
    "Words = All the words of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb25adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ritik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ritik\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \" Hello , I am RITIK. Today i have started NLP. Lets code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd5111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e6cfdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt',download_dir=r'C:\\nltk_data')\n",
    "nltk.data.path.append(r'C:\\nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35f18d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Ritik.', \"Today you've fixed the error.\", 'Now everything works fine.']\n"
     ]
    }
   ],
   "source": [
    "#paragraph-->sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "corpus = \"Hello Ritik. Today you've fixed the error. Now everything works fine.\"\n",
    "sentences = sent_tokenize(corpus)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0dbfc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paragraph---> words\n",
    "# senetenc --> words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0004ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Ritik',\n",
       " '.',\n",
       " 'Today',\n",
       " 'you',\n",
       " \"'ve\",\n",
       " 'fixed',\n",
       " 'the',\n",
       " 'error',\n",
       " '.',\n",
       " 'Now',\n",
       " 'everything',\n",
       " 'works',\n",
       " 'fine',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in this you've like words are treated as single quantitiy\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0de45121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another method\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b6b443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Ritik',\n",
       " '.',\n",
       " 'Today',\n",
       " 'you',\n",
       " \"'\",\n",
       " 've',\n",
       " 'fixed',\n",
       " 'the',\n",
       " 'error',\n",
       " '.',\n",
       " 'Now',\n",
       " 'everything',\n",
       " 'works',\n",
       " 'fine',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here you've is treated as two different words.\n",
    "# wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb2619a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another method\n",
    "# here full stop is not treated as separate word except the last full stop\n",
    "from nltk.tokenize import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee8e6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordDetokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0a4ffb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"H e l l o   R i t i k .   T o d a y   y o u' v e   f i x e d   t h e   e r r o r .   N o w   e v e r y t h i n g   w o r k s   f i n e.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6efa3",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "it is a process to reduce to its word stem or root word  \n",
    "eg . going,gone,goes --> go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17717309",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"go\", \"goes\", \"going\", \"gone\",\n",
    "    \"run\", \"runs\", \"running\", \"ran\",\n",
    "    \"eat\", \"eats\", \"eating\", \"ate\", \"eaten\",\n",
    "    \"play\", \"plays\", \"playing\", \"played\",\n",
    "    \"make\", \"makes\", \"making\", \"made\",\n",
    "    \"see\", \"sees\", \"seeing\", \"saw\", \"seen\",\n",
    "    \"take\", \"takes\", \"taking\", \"took\", \"taken\",\n",
    "    \"write\", \"writes\", \"writing\", \"wrote\", \"written\",\n",
    "    \"come\", \"comes\", \"coming\", \"came\",\n",
    "    \"read\", \"reads\", \"reading\", \"read\",\n",
    "    \"walk\", \"walks\", \"walking\", \"walked\",\n",
    "    \"buy\", \"buys\", \"buying\", \"bought\",\n",
    "    \"sleep\", \"sleeps\", \"sleeping\", \"slept\",\n",
    "    \"drink\", \"drinks\", \"drinking\", \"drank\", \"drunk\",\n",
    "    \"give\", \"gives\", \"giving\", \"gave\", \"given\",\n",
    "    \"think\", \"thinks\", \"thinking\", \"thought\",\n",
    "    \"begin\", \"begins\", \"beginning\", \"began\", \"begun\",\n",
    "    \"drive\", \"drives\", \"driving\", \"drove\", \"driven\",\n",
    "    \"speak\", \"speaks\", \"speaking\", \"spoke\", \"spoken\",\n",
    "    \"fly\", \"flies\", \"flying\", \"flew\", \"flown\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d1866",
   "metadata": {},
   "source": [
    "# PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95c4e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. porter stemmmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "837ab8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go-->go\n",
      "goes-->goe\n",
      "going-->go\n",
      "gone-->gone\n",
      "run-->run\n",
      "runs-->run\n",
      "running-->run\n",
      "ran-->ran\n",
      "eat-->eat\n",
      "eats-->eat\n",
      "eating-->eat\n",
      "ate-->ate\n",
      "eaten-->eaten\n",
      "play-->play\n",
      "plays-->play\n",
      "playing-->play\n",
      "played-->play\n",
      "make-->make\n",
      "makes-->make\n",
      "making-->make\n",
      "made-->made\n",
      "see-->see\n",
      "sees-->see\n",
      "seeing-->see\n",
      "saw-->saw\n",
      "seen-->seen\n",
      "take-->take\n",
      "takes-->take\n",
      "taking-->take\n",
      "took-->took\n",
      "taken-->taken\n",
      "write-->write\n",
      "writes-->write\n",
      "writing-->write\n",
      "wrote-->wrote\n",
      "written-->written\n",
      "come-->come\n",
      "comes-->come\n",
      "coming-->come\n",
      "came-->came\n",
      "read-->read\n",
      "reads-->read\n",
      "reading-->read\n",
      "read-->read\n",
      "walk-->walk\n",
      "walks-->walk\n",
      "walking-->walk\n",
      "walked-->walk\n",
      "buy-->buy\n",
      "buys-->buy\n",
      "buying-->buy\n",
      "bought-->bought\n",
      "sleep-->sleep\n",
      "sleeps-->sleep\n",
      "sleeping-->sleep\n",
      "slept-->slept\n",
      "drink-->drink\n",
      "drinks-->drink\n",
      "drinking-->drink\n",
      "drank-->drank\n",
      "drunk-->drunk\n",
      "give-->give\n",
      "gives-->give\n",
      "giving-->give\n",
      "gave-->gave\n",
      "given-->given\n",
      "think-->think\n",
      "thinks-->think\n",
      "thinking-->think\n",
      "thought-->thought\n",
      "begin-->begin\n",
      "begins-->begin\n",
      "beginning-->begin\n",
      "began-->began\n",
      "begun-->begun\n",
      "drive-->drive\n",
      "drives-->drive\n",
      "driving-->drive\n",
      "drove-->drove\n",
      "driven-->driven\n",
      "speak-->speak\n",
      "speaks-->speak\n",
      "speaking-->speak\n",
      "spoke-->spoke\n",
      "spoken-->spoken\n",
      "fly-->fli\n",
      "flies-->fli\n",
      "flying-->fli\n",
      "flew-->flew\n",
      "flown-->flown\n"
     ]
    }
   ],
   "source": [
    "# it does not give proper root word for some words\n",
    "for word in words:\n",
    "    print(word + \"-->\" + stemming.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a133250",
   "metadata": {},
   "source": [
    "# RegexpStemmer\n",
    "it uses regular expression to remove prefix or suffix from a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd42dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer = RegexpStemmer('ing$|s$|able$',min =4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "487450f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('eating')\n",
    "reg_stemmer.stem('programmable')\n",
    "reg_stemmer.stem('eats')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5075caf",
   "metadata": {},
   "source": [
    "## Snowball Stemmer\n",
    "Better than Porter stemmer in some ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f8ab131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snow_stem = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e821b28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go-->go\n",
      "goes-->goe\n",
      "going-->go\n",
      "gone-->gone\n",
      "run-->run\n",
      "runs-->run\n",
      "running-->run\n",
      "ran-->ran\n",
      "eat-->eat\n",
      "eats-->eat\n",
      "eating-->eat\n",
      "ate-->ate\n",
      "eaten-->eaten\n",
      "play-->play\n",
      "plays-->play\n",
      "playing-->play\n",
      "played-->play\n",
      "make-->make\n",
      "makes-->make\n",
      "making-->make\n",
      "made-->made\n",
      "see-->see\n",
      "sees-->see\n",
      "seeing-->see\n",
      "saw-->saw\n",
      "seen-->seen\n",
      "take-->take\n",
      "takes-->take\n",
      "taking-->take\n",
      "took-->took\n",
      "taken-->taken\n",
      "write-->write\n",
      "writes-->write\n",
      "writing-->write\n",
      "wrote-->wrote\n",
      "written-->written\n",
      "come-->come\n",
      "comes-->come\n",
      "coming-->come\n",
      "came-->came\n",
      "read-->read\n",
      "reads-->read\n",
      "reading-->read\n",
      "read-->read\n",
      "walk-->walk\n",
      "walks-->walk\n",
      "walking-->walk\n",
      "walked-->walk\n",
      "buy-->buy\n",
      "buys-->buy\n",
      "buying-->buy\n",
      "bought-->bought\n",
      "sleep-->sleep\n",
      "sleeps-->sleep\n",
      "sleeping-->sleep\n",
      "slept-->slept\n",
      "drink-->drink\n",
      "drinks-->drink\n",
      "drinking-->drink\n",
      "drank-->drank\n",
      "drunk-->drunk\n",
      "give-->give\n",
      "gives-->give\n",
      "giving-->give\n",
      "gave-->gave\n",
      "given-->given\n",
      "think-->think\n",
      "thinks-->think\n",
      "thinking-->think\n",
      "thought-->thought\n",
      "begin-->begin\n",
      "begins-->begin\n",
      "beginning-->begin\n",
      "began-->began\n",
      "begun-->begun\n",
      "drive-->drive\n",
      "drives-->drive\n",
      "driving-->drive\n",
      "drove-->drove\n",
      "driven-->driven\n",
      "speak-->speak\n",
      "speaks-->speak\n",
      "speaking-->speak\n",
      "spoke-->spoke\n",
      "spoken-->spoken\n",
      "fly-->fli\n",
      "flies-->fli\n",
      "flying-->fli\n",
      "flew-->flew\n",
      "flown-->flown\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"-->\" + snow_stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd4f3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
